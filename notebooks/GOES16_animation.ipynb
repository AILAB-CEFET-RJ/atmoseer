{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/goes16_utils.py\n",
    "# Training: Python and GOES-R Imagery: Script 8 - Functions for download data from AWS\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Required modules\n",
    "import os                                # Miscellaneous operating system interfaces\n",
    "import numpy as np                       # Import the Numpy package\n",
    "import colorsys                          # To make convertion of colormaps\n",
    "import boto3                             # Amazon Web Services (AWS) SDK for Python\n",
    "from botocore import UNSIGNED            # boto3 config\n",
    "from botocore.config import Config       # boto3 config\n",
    "import math                              # Mathematical functions\n",
    "from datetime import datetime            # Basic Dates and time types\n",
    "from osgeo import osr                    # Python bindings for GDAL\n",
    "from osgeo import gdal                   # Python bindings for GDAL\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def loadCPT(path):\n",
    "\n",
    "    try:\n",
    "        f = open(path)\n",
    "    except:\n",
    "        print (\"File \", path, \"not found\")\n",
    "        return None\n",
    "\n",
    "    lines = f.readlines()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    x = np.array([])\n",
    "    r = np.array([])\n",
    "    g = np.array([])\n",
    "    b = np.array([])\n",
    "\n",
    "    colorModel = 'RGB'\n",
    "\n",
    "    for l in lines:\n",
    "        ls = l.split()\n",
    "        if l[0] == '#':\n",
    "            if ls[-1] == 'HSV':\n",
    "                colorModel = 'HSV'\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        if ls[0] == 'B' or ls[0] == 'F' or ls[0] == 'N':\n",
    "            pass\n",
    "        else:\n",
    "            x=np.append(x,float(ls[0]))\n",
    "            r=np.append(r,float(ls[1]))\n",
    "            g=np.append(g,float(ls[2]))\n",
    "            b=np.append(b,float(ls[3]))\n",
    "            xtemp = float(ls[4])\n",
    "            rtemp = float(ls[5])\n",
    "            gtemp = float(ls[6])\n",
    "            btemp = float(ls[7])\n",
    "\n",
    "        x=np.append(x,xtemp)\n",
    "        r=np.append(r,rtemp)\n",
    "        g=np.append(g,gtemp)\n",
    "        b=np.append(b,btemp)\n",
    "\n",
    "    if colorModel == 'HSV':\n",
    "        for i in range(r.shape[0]):\n",
    "            rr, gg, bb = colorsys.hsv_to_rgb(r[i]/360.,g[i],b[i])\n",
    "        r[i] = rr ; g[i] = gg ; b[i] = bb\n",
    "\n",
    "    if colorModel == 'RGB':\n",
    "        r = r/255.0\n",
    "        g = g/255.0\n",
    "        b = b/255.0\n",
    "\n",
    "    xNorm = (x - x[0])/(x[-1] - x[0])\n",
    "\n",
    "    red   = []\n",
    "    blue  = []\n",
    "    green = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        red.append([xNorm[i],r[i],r[i]])\n",
    "        green.append([xNorm[i],g[i],g[i]])\n",
    "        blue.append([xNorm[i],b[i],b[i]])\n",
    "\n",
    "    colorDict = {'red': red, 'green': green, 'blue': blue}\n",
    "\n",
    "    return colorDict\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def download_CMI(yyyymmddhhmn, band, path_dest):\n",
    "\n",
    "  os.makedirs(path_dest, exist_ok=True)\n",
    "\n",
    "  year = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%Y')\n",
    "  day_of_year = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%j')\n",
    "  hour = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%H')\n",
    "  min = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%M')\n",
    "\n",
    "  # AMAZON repository information\n",
    "  # https://noaa-goes16.s3.amazonaws.com/index.html\n",
    "  bucket_name = 'noaa-goes16'\n",
    "  product_name = 'ABI-L2-CMIPF'\n",
    "\n",
    "  # Initializes the S3 client\n",
    "  s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # File structure\n",
    "  prefix = f'{product_name}/{year}/{day_of_year}/{hour}/OR_{product_name}-M6C{int(band):02.0f}_G16_s{year}{day_of_year}{hour}{min}'\n",
    "\n",
    "  # Seach for the file on the server\n",
    "  s3_result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
    "\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # Check if there are files available\n",
    "  if 'Contents' not in s3_result:\n",
    "    # There are no files\n",
    "    print(f'No files found for the date: {yyyymmddhhmn}, Band-{band}')\n",
    "    return -1\n",
    "  else:\n",
    "    # There are files\n",
    "    for obj in s3_result['Contents']:\n",
    "      key = obj['Key']\n",
    "      # Print the file name\n",
    "      file_name = key.split('/')[-1].split('.')[0]\n",
    "\n",
    "      # Download the file\n",
    "      if os.path.exists(f'{path_dest}/{file_name}.nc'):\n",
    "        print(f'File {path_dest}/{file_name}.nc exists')\n",
    "      else:\n",
    "        print(f'Downloading file {path_dest}/{file_name}.nc')\n",
    "        s3_client.download_file(bucket_name, key, f'{path_dest}/{file_name}.nc')\n",
    "  return f'{file_name}'\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def download_PROD(yyyymmddhhmn, product_name, path_dest):\n",
    "\n",
    "  # os.makedirs(path_dest, exist_ok=True)\n",
    "\n",
    "  year = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%Y')\n",
    "  day_of_year = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%j')\n",
    "  hour = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%H')\n",
    "  min = datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%M')\n",
    "\n",
    "  # AMAZON repository information\n",
    "  # https://noaa-goes16.s3.amazonaws.com/index.html\n",
    "  bucket_name = 'noaa-goes16'\n",
    "\n",
    "  # Initializes the S3 client\n",
    "  # s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # File structure\n",
    "  prefix = f'{product_name}/{year}/{day_of_year}/{hour}/OR_{product_name}-M6_G16_s{year}{day_of_year}{hour}{min}'\n",
    "\n",
    "  # Seach for the file on the server\n",
    "  s3_result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
    "\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # Check if there are files available\n",
    "  if 'Contents' not in s3_result:\n",
    "    # There are no files\n",
    "    print(f'No files found for the date: {yyyymmddhhmn}, Product: {product_name}')\n",
    "    return -1\n",
    "  else:\n",
    "    # There are files\n",
    "    for obj in s3_result['Contents']:\n",
    "      key = obj['Key']\n",
    "      # Print the file name\n",
    "      file_name = key.split('/')[-1].split('.')[0]\n",
    "\n",
    "      # print(f'File name: {file_name}')\n",
    "\n",
    "      # Download the file\n",
    "      if os.path.exists(f'{path_dest}/{file_name}.nc'):\n",
    "        print(f'File {path_dest}/{file_name}.nc exists')\n",
    "      else:\n",
    "        # print(f'Downloading file {path_dest}/{file_name}.nc')\n",
    "        s3_client.download_file(bucket_name, key, f'{path_dest}/{file_name}.nc')\n",
    "  return f'{file_name}'\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def download_GLM(yyyymmddhhmnss, path_dest):\n",
    "\n",
    "  os.makedirs(path_dest, exist_ok=True)\n",
    "\n",
    "  year = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%Y')\n",
    "  day_of_year = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%j')\n",
    "  hour = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%H')\n",
    "  min = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%M')\n",
    "  seg = datetime.strptime(yyyymmddhhmnss, '%Y%m%d%H%M%S').strftime('%S')\n",
    "\n",
    "  # AMAZON repository information\n",
    "  # https://noaa-goes16.s3.amazonaws.com/index.html\n",
    "  bucket_name = 'noaa-goes16'\n",
    "\n",
    "  # Initializes the S3 client\n",
    "  s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # File structure\n",
    "  product_name = \"GLM-L2-LCFA\"\n",
    "  prefix = f'{product_name}/{year}/{day_of_year}/{hour}/OR_{product_name}_G16_s{year}{day_of_year}{hour}{min}{seg}'\n",
    "\n",
    "  # Seach for the file on the server\n",
    "  s3_result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
    "\n",
    "  #-----------------------------------------------------------------------------------------------------------\n",
    "  # Check if there are files available\n",
    "  if 'Contents' not in s3_result:\n",
    "    # There are no files\n",
    "    print(f'No files found for the date: {yyyymmddhhmnss}, Product-{product_name}')\n",
    "    return -1\n",
    "  else:\n",
    "    # There are files\n",
    "    for obj in s3_result['Contents']:\n",
    "      key = obj['Key']\n",
    "      # Print the file name\n",
    "      file_name = key.split('/')[-1].split('.')[0]\n",
    "\n",
    "      # Download the file\n",
    "      if os.path.exists(f'{path_dest}/{file_name}.nc'):\n",
    "        print(f'File {path_dest}/{file_name}.nc exists')\n",
    "      else:\n",
    "        print(f'Downloading file {path_dest}/{file_name}.nc')\n",
    "        s3_client.download_file(bucket_name, key, f'{path_dest}/{file_name}.nc')\n",
    "  return f'{file_name}'\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Functions to convert lat / lon extent to array indices\n",
    "def geo2grid(lat, lon, nc):\n",
    "\n",
    "    # Apply scale and offset\n",
    "    xscale, xoffset = nc.variables['x'].scale_factor, nc.variables['x'].add_offset\n",
    "    yscale, yoffset = nc.variables['y'].scale_factor, nc.variables['y'].add_offset\n",
    "\n",
    "    x, y = latlon2xy(lat, lon)\n",
    "    col = (x - xoffset)/xscale\n",
    "    lin = (y - yoffset)/yscale\n",
    "    return int(lin), int(col)\n",
    "\n",
    "def latlon2xy(lat, lon):\n",
    "    # goes_imagery_projection:semi_major_axis\n",
    "    req = 6378137 # meters\n",
    "    #  goes_imagery_projection:inverse_flattening\n",
    "    invf = 298.257222096\n",
    "    # goes_imagery_projection:semi_minor_axis\n",
    "    rpol = 6356752.31414 # meters\n",
    "    e = 0.0818191910435\n",
    "    # goes_imagery_projection:perspective_point_height + goes_imagery_projection:semi_major_axis\n",
    "    H = 42164160 # meters\n",
    "    # goes_imagery_projection: longitude_of_projection_origin\n",
    "    lambda0 = -1.308996939\n",
    "\n",
    "    # Convert to radians\n",
    "    latRad = lat * (math.pi/180)\n",
    "    lonRad = lon * (math.pi/180)\n",
    "\n",
    "    # (1) geocentric latitude\n",
    "    Phi_c = math.atan(((rpol * rpol)/(req * req)) * math.tan(latRad))\n",
    "    # (2) geocentric distance to the point on the ellipsoid\n",
    "    rc = rpol/(math.sqrt(1 - ((e * e) * (math.cos(Phi_c) * math.cos(Phi_c)))))\n",
    "    # (3) sx\n",
    "    sx = H - (rc * math.cos(Phi_c) * math.cos(lonRad - lambda0))\n",
    "    # (4) sy\n",
    "    sy = -rc * math.cos(Phi_c) * math.sin(lonRad - lambda0)\n",
    "    # (5)\n",
    "    sz = rc * math.sin(Phi_c)\n",
    "\n",
    "    # x,y\n",
    "    x = math.asin((-sy)/math.sqrt((sx*sx) + (sy*sy) + (sz*sz)))\n",
    "    y = math.atan(sz/sx)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Function to convert lat / lon extent to GOES-16 extents\n",
    "def convertExtent2GOESProjection(extent):\n",
    "    # GOES-16 viewing point (satellite position) height above the earth\n",
    "    GOES16_HEIGHT = 35786023.0\n",
    "    # GOES-16 longitude position\n",
    "    GOES16_LONGITUDE = -75.0\n",
    "\n",
    "    a, b = latlon2xy(extent[1], extent[0])\n",
    "    c, d = latlon2xy(extent[3], extent[2])\n",
    "    return (a * GOES16_HEIGHT, c * GOES16_HEIGHT, b * GOES16_HEIGHT, d * GOES16_HEIGHT)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Function to reproject the data\n",
    "def reproject(file_name, ncfile, array, extent, undef):\n",
    "\n",
    "    # Read the original file projection and configure the output projection\n",
    "    source_prj = osr.SpatialReference()\n",
    "    source_prj.ImportFromProj4(ncfile.GetProjectionRef())\n",
    "\n",
    "    target_prj = osr.SpatialReference()\n",
    "    target_prj.ImportFromProj4(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "\n",
    "    # Reproject the data\n",
    "    GeoT = ncfile.GetGeoTransform()\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    raw = driver.Create('raw', array.shape[0], array.shape[1], 1, gdal.GDT_Float32)\n",
    "    raw.SetGeoTransform(GeoT)\n",
    "    raw.GetRasterBand(1).WriteArray(array)\n",
    "\n",
    "    # Define the parameters of the output file\n",
    "    kwargs = {'format': 'netCDF', \\\n",
    "            'srcSRS': source_prj, \\\n",
    "            'dstSRS': target_prj, \\\n",
    "            'outputBounds': (extent[0], extent[3], extent[2], extent[1]), \\\n",
    "            'outputBoundsSRS': target_prj, \\\n",
    "            'outputType': gdal.GDT_Float32, \\\n",
    "            'srcNodata': undef, \\\n",
    "            'dstNodata': 'nan', \\\n",
    "            'resampleAlg': gdal.GRA_NearestNeighbour}\n",
    "\n",
    "    # Write the reprojected file on disk\n",
    "    gdal.Warp(file_name, raw, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/goes16/'\n",
    "file_name = 'OR_ABI-L2-DSIF-M6_G16_s20220911700205_e20220911709513_c20220911711480.nc'\n",
    "\n",
    "# Region of Interest - Rio de Janeiro municipality\n",
    "extent = [-43.890602827150, -23.1339033365138, -43.0483514573222, -22.64972474827293]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetCDF to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset              # Read / Write NetCDF4 files\n",
    "import matplotlib.pyplot as plt          # Plotting library\n",
    "from datetime import datetime            # Basic Dates and time types\n",
    "import cartopy, cartopy.crs as ccrs      # Plot maps\n",
    "import os                                # Miscellaneous operating system interfaces\n",
    "import pickle\n",
    "\n",
    "def netcdf2pickle(path, file_name):\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Open the GOES-R image\n",
    "    full_disk_ds = Dataset(path+file_name)\n",
    "\n",
    "    print(full_disk_ds)\n",
    "\n",
    "    # Convert lat/lon to grid-coordinates\n",
    "    lly, llx = geo2grid(extent[1], extent[0], full_disk_ds)\n",
    "    ury, urx = geo2grid(extent[3], extent[2], full_disk_ds)\n",
    "\n",
    "    dict_indices = {}\n",
    "    for instability_index_name in ['CAPE', 'LI', 'TT', 'SI', 'KI']:\n",
    "        # Get the pixel values\n",
    "        data = full_disk_ds.variables[instability_index_name][ury:lly, llx:urx]\n",
    "        dict_indices[instability_index_name] = data\n",
    "\n",
    "    data = full_disk_ds.variables['DQF_Overall'][ury:lly, llx:urx]\n",
    "    dict_indices['DQF_Overall'] = data\n",
    "\n",
    "    print(f'Shape of sliced data: {data.shape}')\n",
    "    \n",
    "    # create a file to store the pickled data\n",
    "    temp = os.path.splitext(file_name)  # given '/home/user/somefile.nc',  returns ('/home/user/somefile', '.nc')\n",
    "    pkl_file_name = f'{temp[0]}.pkl'\n",
    "    pkl_file = open(pkl_file_name, 'wb')\n",
    "\n",
    "    # Extract date\n",
    "    # date = (datetime.strptime(file.time_coverage_start, '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "    # print(date)\n",
    "\n",
    "    print(f'Created pickle file: {pkl_file_name}')\n",
    "\n",
    "    # dump information to that file\n",
    "    pickle.dump(dict_indices, pkl_file)\n",
    "\n",
    "    # close the file\n",
    "    pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf2pickle(path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def min_max_normalize_masked_array(masked_array: np.ma.MaskedArray):\n",
    "    # Ensure the input is a MaskedArray\n",
    "    if not isinstance(masked_array, np.ma.MaskedArray):\n",
    "        raise ValueError(\"Input must be a numpy.ma.MaskedArray\")\n",
    "\n",
    "    # Calculate the min and max values ignoring the masked elements\n",
    "    min_val = masked_array.min()\n",
    "    max_val = masked_array.max()\n",
    "\n",
    "    # Avoid division by zero in case max_val equals min_val\n",
    "    if max_val == min_val:\n",
    "        return np.ma.masked_array(np.zeros_like(masked_array), mask=masked_array.mask)\n",
    "\n",
    "    # Apply the min-max normalization\n",
    "    normalized_array = (masked_array - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_array\n",
    "\n",
    "def split_filename(full_filename):\n",
    "    # Extract the directory path and the filename with extension\n",
    "    dir_path, filename_with_ext = os.path.split(full_filename)\n",
    "    \n",
    "    # Extract the base name and the extension\n",
    "    base_name, file_ext = os.path.splitext(filename_with_ext)\n",
    "    \n",
    "    return (dir_path, base_name, file_ext)\n",
    "\n",
    "def pkl2png(file_name: str, product_name: str, extent: list, output_folder: str, show_image: bool = False):\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open(file_name, 'rb')\n",
    "\n",
    "    dsif_data = pickle.load(file)\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "\n",
    "    data = dsif_data[product_name]\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Compute data-extent in GOES projection-coordinates\n",
    "    img_extent = convertExtent2GOESProjection(extent)\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Choose the plot size (width x height, in inches)\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Use the Geostationary projection in cartopy\n",
    "    ax = plt.axes(projection=ccrs.Geostationary(central_longitude=-75.0, satellite_height=35786023.0))\n",
    "\n",
    "    # Define the color scale based on the channel\n",
    "    colormap = \"jet\" # White to black for IR channels\n",
    "    \n",
    "    data = min_max_normalize_masked_array(data)\n",
    "\n",
    "    # Plot the image\n",
    "    img = ax.imshow(data, origin='upper', extent=img_extent, cmap=colormap)\n",
    "\n",
    "    # Add coastlines, borders and gridlines\n",
    "    ax.coastlines(resolution='10m', color='black', linewidth=0.8)\n",
    "    ax.add_feature(cartopy.feature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "    ax.gridlines(color='black', alpha=0.5, linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Add a colorbar\n",
    "    plt.colorbar(img, label='CAPE', extend='both', orientation='horizontal', pad=0.05, fraction=0.05)\n",
    "\n",
    "    # Extract the date\n",
    "    # date = (datetime.strptime(file.time_coverage_start, '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(f'GOES-16 {product_name}', fontweight='bold', fontsize=10, loc='left')\n",
    "    plt.title('Reg.: ' + str(extent) , fontsize=10, loc='right')\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Save the image\n",
    "    dir_path, base_name, file_ext = split_filename(file_name)\n",
    "    png_file_name = f'{output_folder}/{base_name}.png'\n",
    "    print()\n",
    "    plt.savefig(png_file_name, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "    print(f'Created pickle file: {png_file_name}')\n",
    "\n",
    "    if show_image:\n",
    "        # Show the image\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def split_filename(full_filename):\n",
    "#     # Extract the directory path and the filename with extension\n",
    "#     dir_path, filename_with_ext = os.path.split(full_filename)\n",
    "    \n",
    "#     # Extract the base name and the extension\n",
    "#     base_name, file_ext = os.path.splitext(filename_with_ext)\n",
    "    \n",
    "#     return (dir_path, base_name, file_ext)\n",
    "\n",
    "# # Example usage\n",
    "# full_filename = '/home/user/somefile.pkl'\n",
    "# components = split_filename(full_filename)\n",
    "# print(components)  # Output: ('/home/user', 'somefile', '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '../data/goes16/dsif_data/202401010050.pkl'\n",
    "filename = 'OR_ABI-L2-DSIF-M6_G16_s20220911700205_e20220911709513_c20220911711480.pkl'\n",
    "output_folder = '../data/goes16/dsif_data/png'\n",
    "pkl2png(filename, 'SI', extent, output_folder, show_image = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_timestamps(initial_timestamp, final_timestamp, interval_in_minutes = 10):\n",
    "    # Parse the input timestamps to datetime objects\n",
    "    initial_dt = datetime.strptime(initial_timestamp, '%Y%m%d%H%M')\n",
    "    final_dt = datetime.strptime(final_timestamp, '%Y%m%d%H%M')\n",
    "\n",
    "    # Generate timestamps in 10-minute intervals\n",
    "    timestamps = []\n",
    "    current_dt = initial_dt\n",
    "\n",
    "    while current_dt <= final_dt:\n",
    "        timestamps.append(current_dt.strftime('%Y%m%d%H%M'))\n",
    "        current_dt += timedelta(minutes=interval_in_minutes)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "def get_frame(data, extent):\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Compute data-extent in GOES projection-coordinates\n",
    "    img_extent = convertExtent2GOESProjection(extent)\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # Choose the plot size (width x height, in inches)\n",
    "    # plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Use the Geostationary projection in cartopy\n",
    "    ax = plt.axes(projection=ccrs.Geostationary(central_longitude=-75.0, satellite_height=35786023.0))\n",
    "\n",
    "    # Define the color scale based on the channel\n",
    "    colormap = \"jet\" # White to black for IR channels\n",
    "\n",
    "    # Plot the image\n",
    "    img = ax.imshow(data, origin='upper', extent=img_extent, cmap=colormap, animated = True)\n",
    "\n",
    "    # Add coastlines, borders and gridlines\n",
    "    ax.coastlines(resolution='10m', color='black', linewidth=0.8)\n",
    "    ax.add_feature(cartopy.feature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "    ax.gridlines(color='black', alpha=0.5, linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Add a colorbar\n",
    "    # plt.colorbar(img, label='CAPE', extend='both', orientation='horizontal', pad=0.05, fraction=0.05)\n",
    "\n",
    "    # plt.close()\n",
    "    \n",
    "    return img\n",
    "    return plt.imgshow()\n",
    "\n",
    "def gen_animation(initial_timestamp: str, final_timestamp: str, input_folder: str, product_name: str, extent: list, out_file_name: str):\n",
    "    def update_frame(frame_number, img, images):\n",
    "        \"\"\"Update the image for each frame of the animation.\"\"\"\n",
    "        img.set_array(images[frame_number])\n",
    "        return img,\n",
    "    \n",
    "    # fig = plt.figure()\n",
    "\n",
    "    i_timestamp = int(initial_timestamp)\n",
    "    f_timestamp = int(final_timestamp)\n",
    "\n",
    "    assert i_timestamp <= f_timestamp\n",
    "    \n",
    "    timestamps = generate_timestamps(initial_timestamp, final_timestamp, interval_in_minutes = 10)\n",
    "\n",
    "    # images is a list of artists to draw at each frame\n",
    "    images = []\n",
    "    for timestamp in timestamps:\n",
    "        # print(f'Current timestamp: {timestamp}')\n",
    "\n",
    "        filename = f'{input_folder}/{timestamp}.pkl'\n",
    "\n",
    "        # open a file containing the pickled data\n",
    "        file = open(filename, 'rb')\n",
    "\n",
    "        pickled_data = pickle.load(file)\n",
    "\n",
    "        # close the file\n",
    "        file.close()\n",
    "\n",
    "        data = pickled_data[product_name]\n",
    "\n",
    "        images += [data]\n",
    "\n",
    "    num_images = len(images)\n",
    "    print(num_images)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Use the Geostationary projection in cartopy\n",
    "    ax = plt.axes(projection=ccrs.Geostationary(central_longitude=-75.0, satellite_height=35786023.0))\n",
    "\n",
    "    # See https://stackoverflow.com/questions/67855367/remove-axis-from-animation-artistanimation-python\n",
    "    # ax = fig.add_subplot(111)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # ax.axis('off')\n",
    "\n",
    "    # Add coastlines, borders and gridlines\n",
    "    ax.coastlines(resolution='10m', color='black', linewidth=0.8)\n",
    "    ax.add_feature(cartopy.feature.BORDERS, edgecolor='black', linewidth=0.5)\n",
    "    ax.gridlines(color='black', alpha=0.5, linestyle='--', linewidth=0.5)\n",
    "\n",
    "    img = ax.imshow(images[0], interpolation='none')\n",
    "\n",
    "    # Create the animation\n",
    "    ani = animation.FuncAnimation(fig, update_frame, frames=num_images, fargs=(img, images), blit=True)\n",
    "\n",
    "    # Save the animation\n",
    "    ani.save(out_file_name)\n",
    "    print(f'Created MP4 file: {out_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "\n",
    "# def generate_random_images(num_images, image_shape):\n",
    "#     \"\"\"Generate a list of random images.\"\"\"\n",
    "#     return [np.random.rand(*image_shape) for _ in range(num_images)]\n",
    "\n",
    "# def update_frame(frame_number, img, images):\n",
    "#     \"\"\"Update the image for each frame of the animation.\"\"\"\n",
    "#     img.set_array(images[frame_number])\n",
    "#     return img,\n",
    "\n",
    "# # Parameters\n",
    "# num_images = 10\n",
    "# image_shape = (10, 10)  # Shape of the random images\n",
    "\n",
    "# # Generate random images\n",
    "# images = generate_random_images(num_images, image_shape)\n",
    "\n",
    "# # Create a figure and axis\n",
    "# fig, ax = plt.subplots()\n",
    "# print(type(images), type(images[0]))\n",
    "# img = ax.imshow(images[0], interpolation='none')\n",
    "\n",
    "# # Create the animation\n",
    "# ani = animation.FuncAnimation(fig, update_frame, frames=num_images, fargs=(img, images), blit=True)\n",
    "\n",
    "# # Save the animation as an MP4 file\n",
    "# ani.save('random_images_animation.mp4', writer='ffmpeg', fps=1)\n",
    "\n",
    "# # Display the animation in the notebook (optional)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B.: timestamp format is YYYYMMDDHHMM\n",
    "# plot_anim(initial_timestamp = '202401130000', final_timestamp = '202401132350', input_folder = '../data/goes16/dsif_data')\n",
    "gen_animation(initial_timestamp = '202401130000', \n",
    "                final_timestamp = '202401132350', \n",
    "                input_folder = '../data/goes16/dsif_data', \n",
    "                product_name = 'CAPE', \n",
    "                extent = extent, \n",
    "                out_file_name = '../data/goes16/CAPE_2024_01_01.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def min_max_normalize_masked_array(masked_array):\n",
    "#     # Ensure the input is a MaskedArray\n",
    "#     if not isinstance(masked_array, np.ma.MaskedArray):\n",
    "#         raise ValueError(\"Input must be a numpy.ma.MaskedArray\")\n",
    "\n",
    "#     # Calculate the min and max values ignoring the masked elements\n",
    "#     min_val = masked_array.min()\n",
    "#     max_val = masked_array.max()\n",
    "\n",
    "#     # Avoid division by zero in case max_val equals min_val\n",
    "#     if max_val == min_val:\n",
    "#         return np.ma.masked_array(np.zeros_like(masked_array), mask=masked_array.mask)\n",
    "\n",
    "#     # Apply the min-max normalization\n",
    "#     normalized_array = (masked_array - min_val) / (max_val - min_val)\n",
    "\n",
    "#     return normalized_array\n",
    "\n",
    "# # Example usage\n",
    "# data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# mask = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "# masked_array = np.ma.masked_array(data, mask=mask)\n",
    "\n",
    "# normalized_masked_array = min_max_normalize_masked_array(masked_array)\n",
    "# print(normalized_masked_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('../ABI-L2-DSIF_2024-01-13_to_2024-01-13.parquet')\n",
    "filtered_df = df[~df['CAPE'].isnull()]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('../ABI-L2-TPWF_2024-01-13_to_2024-01-13.parquet')\n",
    "filtered_df = df[~df['TPW'].isnull()]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojection with GDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training: Python and GOES-R Imagery: Script 14 - Reprojection with GDAL\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Required modules\n",
    "from netCDF4 import Dataset                     # Read / Write NetCDF4 files\n",
    "import matplotlib.pyplot as plt                 # Plotting library\n",
    "from datetime import datetime                   # Basic Dates and time types\n",
    "import cartopy, cartopy.crs as ccrs             # Plot maps\n",
    "import os                                       # Miscellaneous operating system interfaces\n",
    "from osgeo import osr                           # Python bindings for GDAL\n",
    "from osgeo import gdal                          # Python bindings for GDAL\n",
    "import numpy as np                              # Scientific computing with Python\n",
    "# from utilities import download_CMI              # Our function for download\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Check if the point is inside the bounding box\n",
    "def is_point_in_bbox(point_lat, point_lon, extent):\n",
    "    min_lon, min_lat, max_lon, max_lat = extent\n",
    "    return min_lon <= point_lon <= max_lon and min_lat <= point_lat <= max_lat\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Input and output directories\n",
    "input = \"../data/goes16/curso_cptec/Samples\"\n",
    "output = \"../data/goes16/curso_cptec/Output\"\n",
    "\n",
    "# Desired extent\n",
    "# extent = [-64.0, -35.0, -35.0, -15.0] # Min lon, Max lon, Min lat, Max lat\n",
    "extent = [-43.890602827150, -23.1339033365138, -43.0483514573222, -22.64972474827293]\n",
    "\n",
    "# Datetime to process\n",
    "yyyymmddhhmn = '202401132300'\n",
    "# yyyymmddhhmn = '202406131800'\n",
    "# yyyymmddhhmn = '202102181800'\n",
    "# yyyymmddhhmn = '202012161300'\n",
    "\n",
    "product_name = 'ABI-L2-DSIF'\n",
    "\n",
    "# Download the file\n",
    "file_name = download_PROD(yyyymmddhhmn, product_name, path_dest=input)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Variable\n",
    "var = 'CAPE'\n",
    "\n",
    "# Open the file\n",
    "img = gdal.Open(f'NETCDF:{input}/{file_name}.nc:' + var)\n",
    "\n",
    "# Data Quality Flag (DQF)\n",
    "dqf = gdal.Open(f'NETCDF:{input}/{file_name}.nc:DQF_Overall')\n",
    "\n",
    "# Read the header metadata\n",
    "metadata = img.GetMetadata()\n",
    "scale = float(metadata.get(var + '#scale_factor'))\n",
    "offset = float(metadata.get(var + '#add_offset'))\n",
    "undef = float(metadata.get(var + '#_FillValue'))\n",
    "dtime = metadata.get('NC_GLOBAL#time_coverage_start')\n",
    "\n",
    "# Load the data\n",
    "ds = img.ReadAsArray(0, 0, img.RasterXSize, img.RasterYSize).astype(float)\n",
    "ds_dqf = dqf.ReadAsArray(0, 0, dqf.RasterXSize, dqf.RasterYSize).astype(float)\n",
    "\n",
    "# Apply the scale and offset\n",
    "ds = (ds * scale + offset)\n",
    "\n",
    "# Apply NaN's where the quality flag is greater than 1\n",
    "# ds[ds_dqf > 1] = np.nan\n",
    "\n",
    "print('type(ds):', type(ds))\n",
    "\n",
    "# Read the original file projection and configure the output projection\n",
    "source_prj = osr.SpatialReference()\n",
    "source_prj.ImportFromProj4(img.GetProjectionRef())\n",
    "\n",
    "target_prj = osr.SpatialReference()\n",
    "target_prj.ImportFromProj4(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n",
    "\n",
    "# Reproject the data\n",
    "GeoT = img.GetGeoTransform()\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "raw = driver.Create('raw', ds.shape[0], ds.shape[1], 1, gdal.GDT_Float32)\n",
    "raw.SetGeoTransform(GeoT)\n",
    "raw.GetRasterBand(1).WriteArray(ds)\n",
    "\n",
    "# Define the parameters of the output file  \n",
    "options = gdal.WarpOptions(format = 'netCDF', \n",
    "          srcSRS = source_prj, \n",
    "          dstSRS = target_prj,\n",
    "          outputBounds = (extent[0], extent[3], extent[2], extent[1]), \n",
    "          outputBoundsSRS = target_prj, \n",
    "          outputType = gdal.GDT_Float32, \n",
    "          srcNodata = undef, \n",
    "          dstNodata = 'nan', \n",
    "        #   xRes = 0.01, \n",
    "        #   yRes = 0.01, \n",
    "          resampleAlg = gdal.GRA_NearestNeighbour)\n",
    "\n",
    "print(options)\n",
    "\n",
    "print('type(raw):', type(raw))\n",
    "\n",
    "# Write the reprojected file on disk\n",
    "filename_reprojected = f'{output}/{file_name}_ret.nc'\n",
    "gdal.Warp(filename_reprojected, raw, options=options)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Open the reprojected GOES-R image\n",
    "file = Dataset(filename_reprojected)\n",
    "\n",
    "sat_data = gdal.Open(f'{filename_reprojected}')\n",
    "# Read number of cols and rows\n",
    "ncol = sat_data.RasterXSize\n",
    "nrow = sat_data.RasterYSize\n",
    "# Load the data\n",
    "sat_array = sat_data.ReadAsArray(0, 0, ncol, nrow).astype(float)\n",
    "# Get geotransform\n",
    "transform = sat_data.GetGeoTransform()\n",
    "\n",
    "# Get the pixel values\n",
    "data = file.variables['Band1'][:]\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Choose the plot size (width x height, in inches)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Use the Geostationary projection in cartopy\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Define the image extent\n",
    "img_extent = [extent[0], extent[2], extent[1], extent[3]]\n",
    "\n",
    "# Define the color scale based on the channel\n",
    "colormap = plt.get_cmap('rainbow').resampled(240)\n",
    "\n",
    "# Plot the image\n",
    "img = ax.imshow(data, origin='upper', extent=img_extent, cmap=colormap, vmin=0, vmax=8000)\n",
    "\n",
    "# Add coastlines, borders and gridlines\n",
    "ax.coastlines(resolution='10m', color='white', linewidth=0.8)\n",
    "ax.add_feature(cartopy.feature.BORDERS, edgecolor='white', linewidth=0.5)\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), color='gray', alpha=1.0, linestyle='--', linewidth=0.25, xlocs=np.arange(-180, 180, 5), ylocs=np.arange(-90, 90, 5), draw_labels=True)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(img, label='CAPE (J/Kg)', extend='both', orientation='horizontal', pad=0.05, fraction=0.05)\n",
    "\n",
    "# Extract date\n",
    "date = (datetime.strptime(dtime, '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "\n",
    "# Add a title\n",
    "plt.title('GOES-16 DSI-CAPE ' + date.strftime('%Y-%m-%d %H:%M') + ' UTC' + '\\nDomain: ' + str(extent), fontsize=10, loc='left')\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Save the image\n",
    "# plt.savefig(f'{output}/Image_14.png', bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "\n",
    "stations_filename = \"../data/ws/WeatherStations.csv\"\n",
    "df_stations = pd.read_csv(stations_filename)\n",
    "\n",
    "for index, row in df_stations.iterrows():\n",
    "    station_id = row['STATION_ID']\n",
    "\n",
    "    # if not station_id.startswith('A652'):\n",
    "    #   continue\n",
    "\n",
    "    point_lat = row['VL_LATITUDE']\n",
    "    point_lon = row['VL_LONGITUDE']\n",
    "    if is_point_in_bbox(point_lat, point_lon, extent):\n",
    "      # Plot the point\n",
    "      x = int((point_lon - transform[0]) / transform[1])\n",
    "      y = int((transform[3] - point_lat) / -transform[5])\n",
    "      pixel_value = sat_array[y,x]\n",
    "      print(f\"Station ID: {station_id}, Lat/Lon: ({point_lat}, {point_lon}), Grid cell: ({y}, {x}), Pixel value: {pixel_value}\")\n",
    "      ax.scatter(point_lon, point_lat, color='black', s=20, edgecolor='black', transform=ccrs.PlateCarree(), zorder=5)\n",
    "\n",
    "# Show the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0,0])\n",
    "print(data[0,1])\n",
    "print(data[0,2])\n",
    "print(data[0,3])\n",
    "print(data[0,4])\n",
    "print(data[0,5])\n",
    "print(data[0,6])\n",
    "print(' ~ ')\n",
    "print(data[1,0])\n",
    "print(data[1,1])\n",
    "print(data[1,2])\n",
    "print(data[1,3])\n",
    "print(data[1,4])\n",
    "print(data[1,5])\n",
    "print(data[1,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_folder_path = '../data/goes16/DSI/'\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Use a for loop to unpack the values returned by os.walk\n",
    "for root, dirs, files in os.walk(input_folder_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Directories: {dirs}\")\n",
    "    print(f\"Files: {files}\")\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmoseer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
